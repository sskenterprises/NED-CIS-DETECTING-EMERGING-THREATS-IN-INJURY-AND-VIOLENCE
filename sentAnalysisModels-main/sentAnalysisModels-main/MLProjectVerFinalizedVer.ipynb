{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLProject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPk4U8NUBwg6"
      },
      "source": [
        "1.   Initialize Parameters\n",
        "2.   Classify and Predict\n",
        "3.   Get Gradint Descent\n",
        "4.   Update weight values\n",
        "5.   Get loss function\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L93i09_GeEty"
      },
      "source": [
        "<h1>NECESSARY LIBRARIES<h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf2CiQEmOWGj",
        "outputId": "b051233d-760b-46c6-e99a-5d72d0d3597f"
      },
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from os import getcwd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import twitter_samples \n",
        "\n",
        "print('Import was successful')\n",
        "# from utils import process_tweet, build_freqs"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Import was successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy3KFBuXeLKr"
      },
      "source": [
        "<h1>1)DATA WRANGLING STAGE<h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr58ibQTeKoS"
      },
      "source": [
        "data = pd.read_csv('Finalcleaneddataset.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "y__ZSjc1gmr0",
        "outputId": "a8bf95a9-fc51-4bba-fd21-e4ba556c90a7"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>date</th>\n",
              "      <th>id</th>\n",
              "      <th>content</th>\n",
              "      <th>username</th>\n",
              "      <th>polarity</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2021-05-23 12:05:20+00:00</td>\n",
              "      <td>1.39644E+18</td>\n",
              "      <td>anyth say islamophobia uk ðÿ ¤ ”</td>\n",
              "      <td>ZAJaffri</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2021-05-07 21:38:56+00:00</td>\n",
              "      <td>1.39078E+18</td>\n",
              "      <td>johnnydepp justiceforjohnnydepp appar up leve...</td>\n",
              "      <td>Stormember</td>\n",
              "      <td>-0.208333</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2021-05-31 22:46:06+00:00</td>\n",
              "      <td>1.3995E+18</td>\n",
              "      <td>happi pride monthâ ™ ¥ ï ¸ � ðÿ � ³ï ¸ � â € ...</td>\n",
              "      <td>TsongaBoii</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2021-04-28 10:19:42+00:00</td>\n",
              "      <td>1.38735E+18</td>\n",
              "      <td>much hate ... hinduphobia</td>\n",
              "      <td>sumitisleo</td>\n",
              "      <td>-0.800000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2021-05-17 18:42:53+00:00</td>\n",
              "      <td>1.39436E+18</td>\n",
              "      <td>never condemn death baloch muslim pakhtoon mo...</td>\n",
              "      <td>singh27manish</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                       date  ...  polarity sentiment\n",
              "0           0  2021-05-23 12:05:20+00:00  ...  0.000000         2\n",
              "1           2  2021-05-07 21:38:56+00:00  ... -0.208333         0\n",
              "2           3  2021-05-31 22:46:06+00:00  ...  0.000000         2\n",
              "3           4  2021-04-28 10:19:42+00:00  ... -0.800000         0\n",
              "4           5  2021-05-17 18:42:53+00:00  ...  0.000000         2\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmMzibB8wMLy",
        "outputId": "4953ed2c-058f-4939-c068-ab467af83c86"
      },
      "source": [
        "data['sentiment'].value_counts()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    25825\n",
              "1    12849\n",
              "0     8020\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPN2EzkfiQYu"
      },
      "source": [
        "<h2>1a) removing the neutral sentiments<h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QIL9M5YtIDE"
      },
      "source": [
        "df = data[(data.sentiment==0) | (data.sentiment==1)]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvwZ_WhwiZb-"
      },
      "source": [
        "<h2>1b) removing the missing values, extracting only \"content\" and \"sentiment\" columns<h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "d3HHhoertIFJ",
        "outputId": "1c8cc5a4-8f33-4264-f810-6aa19f4347cd"
      },
      "source": [
        "df.dropna(inplace=True)\n",
        "df=df[['content', 'sentiment']]\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "# df.head()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>johnnydepp justiceforjohnnydepp appar up leve...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>much hate ... hinduphobia</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>coa mangel attack hamidmir plz tell hamidmir ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>muslim also face islamophobia jew israel terr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tune onlyyouthenewremixesandmor 1hit mauricet...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content  sentiment\n",
              "0   johnnydepp justiceforjohnnydepp appar up leve...          0\n",
              "1                          much hate ... hinduphobia          0\n",
              "2   coa mangel attack hamidmir plz tell hamidmir ...          1\n",
              "3   muslim also face islamophobia jew israel terr...          1\n",
              "4   tune onlyyouthenewremixesandmor 1hit mauricet...          1"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XeVtPrdsDzN"
      },
      "source": [
        "<h2>1c) preparing data and performing train test split of 80-20<h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjgmxZZkxsXv"
      },
      "source": [
        "all_positive_tweets = df[df['sentiment'] == 1].to_numpy()\n",
        "all_negative_tweets = df[df['sentiment'] == 0].to_numpy()\n",
        "\n",
        "# print(all_positive_tweets[0:5],'\\n\\n',all_negative_tweets[0:5])\n",
        "# print('class 0:', class_0.shape)\n",
        "# print('class 1:', class_1.shape)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlrD4Qz9n54Q",
        "outputId": "d353352b-d756-450b-87fb-e68c65c8b0cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_split_0 = int(class_0.shape[0]*0.8)\n",
        "train_split_1 = int(class_1.shape[0]*0.8)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10276"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZoPDZo84FtS"
      },
      "source": [
        "train_pos = all_positive_tweets[:train_split_1]\n",
        "test_pos = all_positive_tweets[train_split_1:]\n",
        "\n",
        "train_neg = all_negative_tweets[:train_split_0]\n",
        "test_neg = all_negative_tweets[train_split_0:]\n",
        "\n",
        "train_x = np.vstack((train_pos, train_neg))  \n",
        "test_x = np.vstack((test_pos,test_neg))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og9wKjFP4FvZ"
      },
      "source": [
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk5sOzL9vx15"
      },
      "source": [
        "<h2>*Note:* additional code incase we need to undersample the give dataset<h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn3uDSDexsgs",
        "outputId": "16b74700-ba15-4175-f5f6-eabd2385d09a"
      },
      "source": [
        "# pos_tweets=[]\n",
        "# neg_tweets=[]\n",
        "# for i in range(len(x_rus)):\n",
        "#   if x_rus[i][1] == 1:\n",
        "#     pos_tweets.append(x_rus[i])\n",
        "#   else:\n",
        "#     neg_tweets.append(x_rus[i]) \n",
        "\n",
        "# print(len(pos_tweets))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nyTPdxQxsbP",
        "outputId": "2fc44fde-fce2-41ff-e24b-43e8d90a969a"
      },
      "source": [
        "# from imblearn.under_sampling import RandomUnderSampler\n",
        "# from collections import Counter\n",
        "\n",
        "# rus = RandomUnderSampler(random_state=42, replacement=True)# fit predictor and target variable\n",
        "# x_rus, y_rus = rus.fit_resample(df, df.category)\n",
        "\n",
        "# print('original dataset shape:', Counter(class_0))\n",
        "# print('Resample dataset shape', Counter(y_rus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "original dataset shape: Counter({'clean_text': 1, 'category': 1})\n",
            "Resample dataset shape Counter({0.0: 55211, 1.0: 55211})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMiwqkpjxS5g"
      },
      "source": [
        "<h2>*Note:* additional code incase some unfimiliar encoding is detected<h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra-9iuQw8cMr"
      },
      "source": [
        "# import chardet # need to import this module to detect the encoding format\n",
        "\n",
        "\n",
        "# for i in range(len(train_x)):\n",
        "#   if isinstance(train_x[i][0], str) != True:\n",
        "#     encode_type = chardet.detect(train_x[i][0])\n",
        "#     print(encode_type)\n",
        "#     Html = train_x[i][0].decode(encode_type['encoding'])\n",
        "#   else:\n",
        "#     pass\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0cvTsdQ4Fxl",
        "outputId": "5ee72c1c-19ac-4718-aa3f-b4e4e6a7ea91"
      },
      "source": [
        "# # Print the shape train and test sets\n",
        "# print(\"train_y.shape = \" + str(train_y.shape))\n",
        "# print(\"test_y.shape = \" + str(test_y.shape))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_y.shape = (16692, 1)\n",
            "test_y.shape = (4174, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb0k43RJxuT8"
      },
      "source": [
        "<h2>1d) processing tweets i.e stemming and cleaning the content<h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaFcT61vL_wk"
      },
      "source": [
        "def process_tweet(tweet):\n",
        "    \"\"\"Process tweet function.\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    \n",
        "    # encode_type = chardet.detect(tweet)\n",
        "    # print(encode_type)\n",
        "    # tweet = tweet.decode(encode_type['encoding'])\n",
        "    \n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    tweet = re.sub(r'http.?://[^\\s]+[\\s]?', '',tweet)\n",
        "    # tweet = re.sub(\"[^a-zA-Z]\", \" \",str(tweet))\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "                word not in string.punctuation):  # remove punctuation\n",
        "            # tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF6MLYeOv8XS"
      },
      "source": [
        "<h2>1e) creating frequency dictionary<h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSLuU2nOL_9H"
      },
      "source": [
        "def build_freqs(tweets, ys):\n",
        "    \"\"\"Build frequencies.\n",
        "    Input:\n",
        "        tweets: a list of tweets\n",
        "        ys: an m x 1 array with the sentiment label of each tweet\n",
        "            (either 0 or 1)\n",
        "    Output:\n",
        "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
        "        frequency\n",
        "    \"\"\"\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "    freqs = {}\n",
        "    for y,tweet in zip(yslist, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "    return freqs\n",
        "    "
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neHnia0p4F2t"
      },
      "source": [
        "content = []\n",
        "for i in range(len(train_x)):\n",
        "  content.append(train_x[i][0])"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iWqMdwL4FzX",
        "outputId": "a9c5186c-6bcb-467d-8ba6-419276d76077"
      },
      "source": [
        "freqs = build_freqs(content, train_y)\n",
        "\n",
        "# check the output\n",
        "print(\"type(freqs) = \" + str(type(freqs)))\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(freqs) = <class 'dict'>\n",
            "len(freqs) = 39022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRNs1i-xHCIU"
      },
      "source": [
        "# freqs"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMSee01S18Tt"
      },
      "source": [
        "<h2>2)ML MODELS<h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83P0pMi52A7F"
      },
      "source": [
        "<h2> 2a) Logistic Regression <h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpaeJUocFdFl"
      },
      "source": [
        "<h3>SIGMOID FUNTION</h3>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjwS-UJNFccM"
      },
      "source": [
        "def sigmoid(z): \n",
        "    '''\n",
        "    Input:  z: is the input (can be a scalar or an array)\n",
        "    Output: h: the sigmoid of z\n",
        "    '''\n",
        "    h = 1/(1 + np.exp(-z))\n",
        "#     if h>=0.5:\n",
        "#         h=1\n",
        "#     else:\n",
        "#         h=0    \n",
        "    return h"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQErNQe6FwE1"
      },
      "source": [
        "<h3>COST FUNCTION AND GRADIENT</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvnwaJtwFcef"
      },
      "source": [
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    '''\n",
        "    Input:\n",
        "        x: matrix of features which is (m,n+1)\n",
        "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
        "        theta: weight vector of dimension (n+1,1)\n",
        "        alpha: learning rate\n",
        "        num_iters: number of iterations you want to train your model for\n",
        "    Output:\n",
        "        J: the final cost\n",
        "        theta: your final weight vector\n",
        "    '''\n",
        "    # get 'm', the number of rows in matrix x\n",
        "    m = x.shape[0]\n",
        "    \n",
        "    for i in range(0, num_iters):\n",
        "        \n",
        "        # get z, the dot product of x and theta\n",
        "        z = np.dot(x,theta)\n",
        "        \n",
        "        # get the sigmoid of z\n",
        "        h = 1/(1 + np.exp(-z))\n",
        "        \n",
        "        # calculate the cost function\n",
        "        J = (-1./m) * ( np.dot(np.transpose(y) , np.log(h)) + np.dot(np.transpose(1-y) , np.log(1-h)) )\n",
        "        \n",
        "        # update the weights theta\n",
        "        theta = (theta) - (alpha/m) * np.dot(np.transpose(x) , (h-y))\n",
        "        \n",
        "    J = float(J)\n",
        "    return J, theta"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwUulfD4GWJQ"
      },
      "source": [
        "<h3>FEATURE EXTRACTION</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYBIu22BFcgt"
      },
      "source": [
        "def extract_features(tweet, freqs):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet: a list of words for one tweet\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "    Output: \n",
        "        x: a feature vector of dimension (1,3)\n",
        "    '''\n",
        "    # process_tweet tokenizes, stems, and removes stopwords\n",
        "    word_l = process_tweet(tweet)\n",
        "    \n",
        "    # 3 elements in the form of a 1 x 3 vector\n",
        "    x = np.zeros((1, 3)) \n",
        "    \n",
        "    #bias term is set to 1\n",
        "    x[0,0] = 1 \n",
        "    \n",
        "    # loop through each word in the list of words\n",
        "    for word in word_l:\n",
        "        \n",
        "        # increment the word count for the positive label 1\n",
        "        x[0,1] += freqs.get((word, 1.0),0)\n",
        "        \n",
        "        # increment the word count for the negative label 0\n",
        "        x[0,2] += freqs.get((word, 0.0),0)\n",
        "        \n",
        "    assert(x.shape == (1, 3))\n",
        "    return x\n"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De4kFosVxgfO"
      },
      "source": [
        "# collect the features 'x' and stack them into a matrix 'X'\n",
        "X = np.zeros((len(content), 3))\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :]= extract_features(content[i], freqs)\n",
        "\n",
        "# print(train_x_1)\n",
        "# training labels corresponding to X\n",
        "Y = train_y\n",
        "\n"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Uvd-a0nJHwf",
        "outputId": "e6ccd336-0baa-48fa-f438-a72033752f19"
      },
      "source": [
        "# Apply gradient descent\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 3000)\n",
        "print(f\"The cost after training is {J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: RuntimeWarning: divide by zero encountered in log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost after training is nan.\n",
            "The resulting vector of weights is [2e-07, 0.0001726, -0.00024203]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0cK49hv2ySN",
        "outputId": "afef9a7e-f80e-41a2-9486-8c41cb4e3d7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(J, theta)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nan [[ 1.98702915e-07]\n",
            " [ 1.72603404e-04]\n",
            " [-2.42031274e-04]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vy-uaXSGofl"
      },
      "source": [
        "<h3>PREDICT TWEETS</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1ZWp5xxFciw"
      },
      "source": [
        "def predict_tweet(tweet, freqs, theta):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet: a string\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        theta: (3,1) vector of weights\n",
        "    Output: \n",
        "        y_pred: the probability of a tweet being positive or negative\n",
        "    '''\n",
        "    \n",
        "    # extract the features of the tweet and store it into x\n",
        "    x = extract_features(tweet,freqs)\n",
        "    \n",
        "    # make the prediction using x and theta\n",
        "    y_pred = sigmoid(np.dot(x,theta))\n",
        "    \n",
        "    return y_pred"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2W9gHp-G42I"
      },
      "source": [
        "<h1>PERFORMANCE</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh6qjn55IPp7"
      },
      "source": [
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzQqpNPfFck0"
      },
      "source": [
        "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        test_x: a list of tweets\n",
        "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "    Output: \n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "    \"\"\"\n",
        "    \n",
        "    y_hat = []\n",
        "    \n",
        "    for tweet in test_x:\n",
        "        # get the label prediction for the tweet\n",
        "        y_pred = predict_tweet(tweet, freqs, theta)\n",
        "        \n",
        "        if y_pred > 0.5:\n",
        "            # append 1.0 to the list\n",
        "            y_hat.append(1.0)\n",
        "        else:\n",
        "            # append 0 to the list\n",
        "            y_hat.append(0.0)\n",
        "\n",
        "    # accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
        "    accuracy = metrics.accuracy_score(test_y,y_hat) \n",
        "    print(\"####################################\\naccuracy:{0}\\n####################################\".format(accuracy))\n",
        "    target_names = ['class 0', 'class 1']\n",
        "    print(classification_report(test_y, y_hat,target_names=target_names))\n",
        "\n",
        "    return y_hat\n",
        "\n"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cih8dW5lJFuM"
      },
      "source": [
        "# my test content\n",
        "\n",
        "test_x_1 = []\n",
        "for i in range(len(test_x)):\n",
        "  test_x_1.append(test_x[i][0])\n"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw0w49PJJc37",
        "outputId": "cabb9a48-3792-4613-f39a-a232f3858dd4"
      },
      "source": [
        "y_hat = test_logistic_regression(test_x_1, test_y, freqs, theta)\n",
        "# print(f\"Logistic regression model's accuracy = {acc:.4f}\")"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####################################\n",
            "accuracy:0.714183037853378\n",
            "####################################\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.69      0.47      0.56      1604\n",
            "     class 1       0.72      0.87      0.79      2570\n",
            "\n",
            "    accuracy                           0.71      4174\n",
            "   macro avg       0.71      0.67      0.67      4174\n",
            "weighted avg       0.71      0.71      0.70      4174\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOVRrUbeHA7k"
      },
      "source": [
        "<h1>ERROR ANALYSIS</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q4Vn3ntG4Jl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "16f20a4d-3e5d-47a9-8c66-52ff10d6bc15"
      },
      "source": [
        "print('Label Predicted Tweet')\n",
        "for x,y in zip(test_x,test_y):\n",
        "    y_hat = predict_tweet(test_x, freqs, theta)\n",
        "\n",
        "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
        "        print('THE TWEET IS:', x)\n",
        "        print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
        "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label Predicted Tweet\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-a1307b724b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Label Predicted Tweet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_tweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-eeb24cc7f270>\u001b[0m in \u001b[0;36mpredict_tweet\u001b[0;34m(tweet, freqs, theta)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# extract the features of the tweet and store it into x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfreqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# make the prediction using x and theta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-d7102f399edd>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(tweet, freqs)\u001b[0m\n\u001b[1;32m      8\u001b[0m     '''\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# process_tweet tokenizes, stems, and removes stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mword_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_tweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# 3 elements in the form of a 1 x 3 vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-2aca6b60751f>\u001b[0m in \u001b[0;36mprocess_tweet\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# tweet = tweet.decode(encode_type['encoding'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\$\\w*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m# remove old style retweet text \"RT\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'^RT[\\s]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nucZ-KkeG4Lh"
      },
      "source": [
        "# Feel free to change the tweet below\n",
        "# my_tweet = 'This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!'\n",
        "my_tweet = 'I think rapists should be hanged because of the henious crimes they conducted against the humainty.'\n",
        "print(process_tweet(my_tweet))\n",
        "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
        "print(y_hat)\n",
        "if y_hat > 0.5:\n",
        "    print('Positive sentiment')\n",
        "else: \n",
        "    print('Negative sentiment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFAzsZ9tG4Np"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW0uTojjAtuj"
      },
      "source": [
        "<h2>2b) Naive Bayes<h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M01vz_HINIw8"
      },
      "source": [
        "def lookup(freqs, word, label):\n",
        "    '''\n",
        "    Input:\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        word: the word to look up\n",
        "        label: the label corresponding to the word\n",
        "    Output:\n",
        "        n: the number of times the word with its corresponding label appears.\n",
        "    '''\n",
        "    n = 0  # freqs.get((word, label), 0)\n",
        "\n",
        "    pair = (word, label)\n",
        "    if (pair in freqs):\n",
        "        n = freqs[pair]\n",
        "\n",
        "    return n"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaC-0PIvG4RQ"
      },
      "source": [
        "def train_naive_bayes(freqs, train_x, train_y):\n",
        "    '''\n",
        "    Input:\n",
        "        freqs: dictionary from (word, label) to how often the word appears\n",
        "        train_x: a list of tweets\n",
        "        train_y: a list of labels correponding to the tweets (0,1)\n",
        "    Output:\n",
        "        logprior: the log prior. (equation 3 above)\n",
        "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
        "    '''\n",
        "    loglikelihood = {}\n",
        "    logprior = 0\n",
        "\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "    # calculate V, the number of unique words in the vocabulary\n",
        "    vocab = set([pair[0] for pair in freqs.keys()])\n",
        "    V = len(vocab)\n",
        "\n",
        "    # calculate N_pos and N_neg\n",
        "    N_pos = N_neg = 0\n",
        "    for pair in freqs.keys():\n",
        "        # if the label is positive (greater than zero)\n",
        "        if pair[1] > 0:\n",
        "\n",
        "            # Increment the number of positive words by the count for this (word, label) pair\n",
        "            N_pos += freqs[pair]\n",
        "\n",
        "        # else, the label is negative\n",
        "        else:\n",
        "\n",
        "            # increment the number of negative words by the count for this (word,label) pair\n",
        "            N_neg += freqs[pair]\n",
        "\n",
        "    # Calculate D, the number of documents\n",
        "    D = len(train_y)\n",
        "\n",
        "    # Calculate D_pos, the number of positive documents (*hint: use sum(<np_array>))\n",
        "    D_pos = (len(list(filter(lambda x: x > 0, train_y))))\n",
        "\n",
        "    # Calculate D_neg, the number of negative documents (*hint: compute using D and D_pos)\n",
        "    D_neg = (len(list(filter(lambda x: x <= 0, train_y))))\n",
        "\n",
        "    # Calculate logprior\n",
        "    logprior = np.log(D_pos) - np.log(D_neg)\n",
        "\n",
        "    # For each word in the vocabulary...\n",
        "    for word in vocab:\n",
        "        # get the positive and negative frequency of the word\n",
        "        freq_pos = lookup(freqs,word,1)\n",
        "        freq_neg = lookup(freqs,word,0)\n",
        "\n",
        "        # calculate the probability that each word is positive, and negative\n",
        "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
        "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
        "\n",
        "        # calculate the log likelihood of the word\n",
        "        loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n",
        "\n",
        "\n",
        "    return logprior, loglikelihood"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhpJ8Hs6FcoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df1368f-010b-43af-f3da-154a2d3b9a29"
      },
      "source": [
        "logprior, loglikelihood = train_naive_bayes(freqs, content, train_y)\n",
        "print(logprior)\n",
        "print(len(loglikelihood))"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4710162086834231\n",
            "30728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cH1gqHeMMXz"
      },
      "source": [
        "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string\n",
        "        logprior: a number\n",
        "        loglikelihood: a dictionary of words mapping to numbers\n",
        "    Output:\n",
        "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
        "\n",
        "    '''\n",
        "    word_l = process_tweet(tweet)\n",
        "\n",
        "    # initialize probability to zero\n",
        "    p = 0\n",
        "\n",
        "    # add the logprior\n",
        "    p += logprior\n",
        "\n",
        "    for word in word_l:\n",
        "\n",
        "        # check if the word exists in the loglikelihood dictionary\n",
        "        if word in loglikelihood:\n",
        "            # add the log likelihood of that word to the probability\n",
        "            p += loglikelihood[word]\n",
        "\n",
        "    return p"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0omrfb8MMZT"
      },
      "source": [
        "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        test_x: A list of tweets\n",
        "        test_y: the corresponding labels for the list of tweets\n",
        "        logprior: the logprior\n",
        "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
        "    Output:\n",
        "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
        "    \"\"\"\n",
        "    accuracy = 0  # return this properly\n",
        "\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    y_hats = []\n",
        "    for tweet in test_x:\n",
        "        # if the prediction is > 0\n",
        "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
        "            # the predicted class is 1\n",
        "            y_hat_i = 1\n",
        "        else:\n",
        "            # otherwise the predicted class is 0\n",
        "            y_hat_i = 0\n",
        "\n",
        "        # append the predicted class to the list y_hats\n",
        "        y_hats.append(y_hat_i)\n",
        "\n",
        "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
        "    # error = np.mean(np.absolute(y_hats-test_y))\n",
        "\n",
        "    # Accuracy is 1 minus the error\n",
        "    accuracy = metrics.accuracy_score(test_y,y_hats)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    target_names = ['class 0', 'class 1']\n",
        "    print(classification_report(test_y, y_hats,target_names=target_names))\n",
        "\n",
        "    return accuracy\n"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d06X-hiqMMcR",
        "outputId": "771bc79c-ee76-4c9a-b9ff-7ce928910c12"
      },
      "source": [
        "print(\"Naive Bayes accuracy = %0.4f\" %\n",
        "      (test_naive_bayes(test_x_1, test_y, logprior, loglikelihood)))"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.80      0.81      0.81      1604\n",
            "     class 1       0.88      0.87      0.88      2570\n",
            "\n",
            "    accuracy                           0.85      4174\n",
            "   macro avg       0.84      0.84      0.84      4174\n",
            "weighted avg       0.85      0.85      0.85      4174\n",
            "\n",
            "Naive Bayes accuracy = 0.8491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuccT52YMMeL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgmnzto9MMhi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}